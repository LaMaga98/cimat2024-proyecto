{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8553322,"sourceType":"datasetVersion","datasetId":5111414},{"sourceId":8553483,"sourceType":"datasetVersion","datasetId":5111521},{"sourceId":8553979,"sourceType":"datasetVersion","datasetId":5111468},{"sourceId":9070004,"sourceType":"datasetVersion","datasetId":5470786}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"$$\\textbf{Proyecto de Verano: PLN aplicado a la Bioinformática}$$\n$$\\textit{Y. Sarahi García Gozález}$$","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport torch\nimport urllib.request\nimport sys\nsys.path.append('/kaggle/input/proyecto-archivos/')\nfrom transformers import BertModel, BertConfig, logging\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score\nimport wandb\nfrom datetime import datetime\nimport yaml\nimport os\nimport shutil\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:22:49.736299Z","iopub.execute_input":"2024-08-02T02:22:49.737203Z","iopub.status.idle":"2024-08-02T02:22:57.594997Z","shell.execute_reply.started":"2024-08-02T02:22:49.737170Z","shell.execute_reply":"2024-08-02T02:22:57.594054Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install torchdrug ","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:22:57.596741Z","iopub.execute_input":"2024-08-02T02:22:57.597044Z","iopub.status.idle":"2024-08-02T02:56:36.294796Z","shell.execute_reply.started":"2024-08-02T02:22:57.597019Z","shell.execute_reply":"2024-08-02T02:56:36.293677Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torchdrug\n  Downloading torchdrug-0.2.1-py3-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from torchdrug) (2.1.2)\nCollecting torch-scatter>=2.0.8 (from torchdrug)\n  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting torch-cluster>=1.5.9 (from torchdrug)\n  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from torchdrug) (5.1.1)\nRequirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.10/site-packages (from torchdrug) (1.26.4)\nCollecting rdkit-pypi>=2020.9 (from torchdrug)\n  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from torchdrug) (3.7.5)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchdrug) (4.66.4)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torchdrug) (3.2.1)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from torchdrug) (1.11.1.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torchdrug) (3.1.2)\nCollecting lmdb (from torchdrug)\n  Downloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nCollecting fair-esm (from torchdrug)\n  Downloading fair_esm-2.0.0-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from rdkit-pypi>=2020.9->torchdrug) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->torchdrug) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->torchdrug) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->torchdrug) (1.13.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->torchdrug) (2024.5.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-cluster>=1.5.9->torchdrug) (1.11.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torchdrug) (2.1.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->torchdrug) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->torchdrug) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->torchdrug) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->torchdrug) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->torchdrug) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->torchdrug) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->torchdrug) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->torchdrug) (1.16.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->torchdrug) (1.3.0)\nDownloading torchdrug-0.2.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: torch-cluster, torch-scatter\n  Building wheel for torch-cluster (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp310-cp310-linux_x86_64.whl size=2045196 sha256=71757398c1d2306d0d2b4b402634d26f20792cde7a0897225cf47624f0a51a5d\n  Stored in directory: /root/.cache/pip/wheels/51/78/c3/536637b3cdcc3313aa5e8851a6c72b97f6a01877e68c7595e3\n  Building wheel for torch-scatter (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=3764941 sha256=1afa370a25cea026236dabcb057ad4c6b9097d20a235ba03931c4154e0054d4b\n  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\nSuccessfully built torch-cluster torch-scatter\nInstalling collected packages: lmdb, fair-esm, torch-scatter, rdkit-pypi, torch-cluster, torchdrug\nSuccessfully installed fair-esm-2.0.0 lmdb-1.5.1 rdkit-pypi-2022.9.5 torch-cluster-1.6.3 torch-scatter-2.1.2 torchdrug-0.2.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torchdrug\nfrom torchdrug.datasets import Fluorescence","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:57:34.414869Z","iopub.execute_input":"2024-08-02T02:57:34.415421Z","iopub.status.idle":"2024-08-02T02:57:35.885625Z","shell.execute_reply.started":"2024-08-02T02:57:34.415384Z","shell.execute_reply":"2024-08-02T02:57:35.884714Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}\\n')","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:57:36.076788Z","iopub.execute_input":"2024-08-02T02:57:36.077537Z","iopub.status.idle":"2024-08-02T02:57:36.082442Z","shell.execute_reply.started":"2024-08-02T02:57:36.077508Z","shell.execute_reply":"2024-08-02T02:57:36.081476Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Device: cuda\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Descargar los datos","metadata":{}},{"cell_type":"code","source":"class PeptideBERTDataset(torch.utils.data.Dataset):\n    def __init__(self, input_ids, attention_masks, labels):\n        self.input_ids = input_ids\n        self.attention_masks = attention_masks\n        self.labels = labels\n\n        self.length = len(self.input_ids)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_id = self.input_ids[idx]\n        attention_mask = self.attention_masks[idx]\n        label = self.labels[idx]\n\n        return {\n            'input_ids': torch.tensor(input_id, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n            'labels': torch.tensor(label, dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:57:38.969110Z","iopub.execute_input":"2024-08-02T02:57:38.969460Z","iopub.status.idle":"2024-08-02T02:57:38.976906Z","shell.execute_reply.started":"2024-08-02T02:57:38.969431Z","shell.execute_reply":"2024-08-02T02:57:38.975934Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"logging.set_verbosity_error()\nimport torch.nn as nn\n# Definimos la clase PeptideBERT, que hereda de torch.nn.Module (la clase base para todas las redes neuronales en PyTorch)\nclass PeptideBERT(nn.Module):\n    def __init__(self, bert_config):\n        super(PeptideBERT, self).__init__()\n\n        # Cargamos el modelo preentrenado\n        self.protbert = BertModel.from_pretrained(\n            'Rostlab/prot_bert_bfd',\n            config=bert_config,\n            ignore_mismatched_sizes=True\n        )\n        \n        # Clasificación con capas adicionales\n        self.head = nn.Sequential(\n            nn.Linear(bert_config.hidden_size, bert_config.hidden_size),\n            nn.ReLU(),  # Función de activación ReLU\n            nn.Dropout(p=0.15),  # Dropout para evitar sobreajuste\n            nn.Linear(bert_config.hidden_size , 1)\n        )\n        \n    def forward(self, inputs, attention_mask):\n        # Pasamos las entradas a través de ProtBert\n        output = self.protbert(inputs, attention_mask=attention_mask)\n        # Usamos la salida de ProtBert como entrada a la capa de clasificación\n        return self.head(output.pooler_output)\n \n","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:57:39.515073Z","iopub.execute_input":"2024-08-02T02:57:39.515418Z","iopub.status.idle":"2024-08-02T02:57:39.524929Z","shell.execute_reply.started":"2024-08-02T02:57:39.515390Z","shell.execute_reply":"2024-08-02T02:57:39.523935Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#criterio de pérdida,optimizador y el planificador de learning rate  para el entrenamiento del modelo\n\ndef cri_opt_sch(config, model):\n    ##criterio de pérdida:MSE\n    criterion = torch.nn.MSELoss()\n    #optimizador AmadW\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config['optim']['lr'])\n    #Scheduler\n    if config['sch']['name'] == 'onecycle':\n        ## Durante el entrenamiento, el learning-rate empieza en un valor inicial, aumenta hasta el valor máximo especificado (max_lr), y luego disminuye nuevamente hacia el final del entrenamiento.\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            max_lr=config['optim']['lr'],\n            epochs=config['epochs'],\n            steps_per_epoch=config['sch']['steps']\n        ) #Ajusta el learning-rate utilizando un ciclo de una sola pasada\n    elif config['sch']['name'] == 'lronplateau':\n        ## ajusta el learning-rate basándose en el rendimiento del modelo. Específicamente, reduce la tasa de aprendizaje cuando una métrica de rendimiento ha dejado de mejorar.\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer,\n            mode='max',\n            factor=config['sch']['factor'],\n            patience=config['sch']['patience']\n        )# Reduce lr cuando la métrica especificada ha dejado de mejorar.\n\n    return criterion, optimizer, scheduler\n","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:57:39.818438Z","iopub.execute_input":"2024-08-02T02:57:39.819120Z","iopub.status.idle":"2024-08-02T02:57:39.826597Z","shell.execute_reply.started":"2024-08-02T02:57:39.819086Z","shell.execute_reply":"2024-08-02T02:57:39.825292Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"###Función que se encarga del proceso de entrenamiento\ndef train(model, dataloader, optimizer, criterion, scheduler, device):\n    model.train()  # Pone el modelo en modo de entrenamiento\n    total_loss = 0.0\n\n    for batch in tqdm(dataloader):  # Itera sobre los lotes de datos en el dataloader\n        inputs = batch['input_ids'].to(device)  # Mueve las entradas al dispositivo (CPU o GPU)\n        attention_mask = batch['attention_mask'].to(device)  # Mueve la máscara de atención al dispositivo\n        labels = batch['labels'].to(device)  # Mueve las etiquetas al dispositivo\n\n        optimizer.zero_grad()  # Resetea los gradientes del optimizador\n\n        logits = model(inputs, attention_mask).squeeze(1)  # Pasa las entradas a través del modelo y ajusta las dimensiones\n        loss = criterion(logits, labels)  # Calcula la pérdida\n\n        loss.backward()  # Calcula los gradientes\n        optimizer.step()  # Actualiza los parámetros del modelo\n        # scheduler.step()  # Si el scheduler es OneCycleLR, ajusta la tasa de aprendizaje en cada paso\n\n        total_loss += loss.item()  # Acumula la pérdida total\n\n    return total_loss / len(dataloader)  # Retorna la pérdida promedio por lote","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:57:40.058652Z","iopub.execute_input":"2024-08-02T02:57:40.059357Z","iopub.status.idle":"2024-08-02T02:57:40.066932Z","shell.execute_reply.started":"2024-08-02T02:57:40.059320Z","shell.execute_reply":"2024-08-02T02:57:40.065757Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import spearmanr\ndef validate(model, dataloader, criterion, device):\n    model.eval()  # Pone el modelo en modo de evaluación\n    total_loss = 0.0\n\n    ground_truth = []\n    predictions = []\n\n    for batch in tqdm(dataloader):  # Itera sobre los lotes de datos en el dataloader\n        inputs = batch['input_ids'].to(device)  # Mueve las entradas al dispositivo\n        attention_mask = batch['attention_mask'].to(device)  # Mueve la máscara de atención al dispositivo\n        labels = batch['labels'].to(device)  # Mueve las etiquetas al dispositivo\n\n        with torch.inference_mode():  # Desactiva el cálculo de gradientes\n            logits = model(inputs, attention_mask).squeeze(1)  # Pasa las entradas a través del modelo\n            loss = criterion(logits, labels)  # Calcula la pérdida\n\n        total_loss += loss.item()  # Acumula la pérdida total\n        predictions.extend(logits.cpu().tolist())  # Añade las predicciones a la lista\n        ground_truth.extend(labels.cpu().tolist())  # Añade las etiquetas reales a la lista\n\n    total_loss = total_loss / len(dataloader)  # Calcula la pérdida promedio\n    # Calcula el coeficiente de Pearson\n    spearmanr_corr, _ = spearmanr(ground_truth, predictions)\n\n    return total_loss,spearmanr_corr  # Retorna la pérdida promedio","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:57:40.241343Z","iopub.execute_input":"2024-08-02T02:57:40.242035Z","iopub.status.idle":"2024-08-02T02:57:40.252544Z","shell.execute_reply.started":"2024-08-02T02:57:40.241995Z","shell.execute_reply":"2024-08-02T02:57:40.251385Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import spearmanr\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom tqdm import tqdm\n\ndef test(model, dataloader, device):\n    model.eval()  # Pone el modelo en modo de evaluación\n\n    ground_truth = []\n    predictions = []\n\n    for batch in tqdm(dataloader):  # Itera sobre los lotes de datos en el dataloader\n        inputs = batch['input_ids'].to(device)  # Mueve las entradas al dispositivo\n        attention_mask = batch['attention_mask'].to(device)  # Mueve la máscara de atención al dispositivo\n        labels = batch['labels']  # Las etiquetas permanecen en la CPU\n\n        with torch.inference_mode():  # Desactiva el cálculo de gradientes\n            logits = model(inputs, attention_mask).squeeze(1)  # Pasa las entradas a través del modelo\n\n        predictions.extend(logits.cpu().tolist())  # Añade las predicciones a la lista\n        ground_truth.extend(labels.tolist())  # Añade las etiquetas reales a la lista\n\n    spearman_corr, _ = spearmanr(ground_truth, predictions)\n\n    return spearman_corr  # Retorna las métricas de regresión y la correlación de Spearman\n","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:57:40.383454Z","iopub.execute_input":"2024-08-02T02:57:40.383772Z","iopub.status.idle":"2024-08-02T02:57:40.391738Z","shell.execute_reply.started":"2024-08-02T02:57:40.383734Z","shell.execute_reply":"2024-08-02T02:57:40.390719Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def train_model(model):\n    print(f'{\"=\"*30}{\"TRAINING\":^20}{\"=\"*30}')\n\n    best_val_spearman = 0.0  # Inicializa la mejor pérdida de validación en infinito\n\n    # Iteramos cada época\n    for epoch in range(config['epochs']):\n\n        # Llamamos a la función de entrenamiento\n        train_loss = train(model, train_data_loader, optimizer, criterion, scheduler, device)\n        # Obtenemos learning rate\n        curr_lr = optimizer.param_groups[0]['lr']\n        # Imprimimos loss de entrenamiento y learning rate\n        print(f'Epoch {epoch+1}/{config[\"epochs\"]} - Train Loss: {train_loss}\\tLR: {curr_lr}')\n        # Imprimimos loss de validación\n        val_loss,val_spearman = validate(model, val_data_loader, criterion, device)\n        print(f'Epoch {epoch+1}/{config[\"epochs\"]} - Validation Loss: {val_loss}\\t Validation spearman: {val_spearman}\\n')\n        # Actualizar el Scheduler:\n        scheduler.step(val_loss)\n\n        # Registrar Métricas con wandb\n        if not config['debug']:\n            wandb.log({\n                'train_loss': train_loss, \n                'val_loss': val_loss, \n                'val_spearman': val_spearman,\n                'lr': curr_lr\n            })\n        # Guardamos mejor modelo\n        if val_spearman >= best_val_spearman and not config['debug']:\n            best_val_spearman = val_spearman\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'train_loss': train_loss,\n                'val_loss': val_loss,\n                'spearman': val_spearman,\n                'lr': curr_lr\n            }, f'{save_dir}/model.pt')\n            print('Model Saved\\n')\n    wandb.finish()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-02T04:19:11.868136Z","iopub.execute_input":"2024-08-02T04:19:11.868526Z","iopub.status.idle":"2024-08-02T04:19:11.881147Z","shell.execute_reply.started":"2024-08-02T04:19:11.868494Z","shell.execute_reply":"2024-08-02T04:19:11.880064Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"Solubilidad","metadata":{}},{"cell_type":"code","source":"ds=Fluorescence(\"f'/kaggle/working/\",lazy=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:57:43.203296Z","iopub.execute_input":"2024-08-02T02:57:43.203645Z","iopub.status.idle":"2024-08-02T02:57:45.336142Z","shell.execute_reply.started":"2024-08-02T02:57:43.203616Z","shell.execute_reply":"2024-08-02T02:57:45.335237Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"02:57:43   Downloading http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/fluorescence.tar.gz to f'/kaggle/working/fluorescence.tar.gz\n02:57:44   Extracting f'/kaggle/working/fluorescence.tar.gz to f'/kaggle/working\n","output_type":"stream"},{"name":"stderr","text":"Constructing proteins from sequences: 100%|██████████| 54025/54025 [00:00<00:00, 144534.49it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"sequences=ds.sequences\ntargets=ds.targets['log_fluorescence']","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:57:45.980933Z","iopub.execute_input":"2024-08-02T02:57:45.981522Z","iopub.status.idle":"2024-08-02T02:57:45.985575Z","shell.execute_reply.started":"2024-08-02T02:57:45.981493Z","shell.execute_reply":"2024-08-02T02:57:45.984555Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"aminoacidos = ['G', 'A', 'S', 'P', 'V', 'T', 'C', 'I', 'L', 'N', 'D', 'Q', 'K', 'E', 'M', 'H', 'F', 'R', 'Y', 'W']\n#diccionario de mapeo\nletter_to_number = {letter: index  for index, letter in enumerate(aminoacidos)}\n# Función para convertir secuencias de letras a secuencias de números\ndef convert_sequences_to_numbers(sequences, mapping):\n    return [[mapping[letter] for letter in seq] for seq in sequences]\n\nsequences_number=convert_sequences_to_numbers(sequences, letter_to_number)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:57:46.560117Z","iopub.execute_input":"2024-08-02T02:57:46.560917Z","iopub.status.idle":"2024-08-02T02:57:47.850012Z","shell.execute_reply.started":"2024-08-02T02:57:46.560886Z","shell.execute_reply":"2024-08-02T02:57:47.849227Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#longitud del string más larga\nmax_length = max(len(s) for s in sequences)\n\nprint(\"La longitud del string más largo es:\", max_length)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:57:47.851603Z","iopub.execute_input":"2024-08-02T02:57:47.852152Z","iopub.status.idle":"2024-08-02T02:57:47.863052Z","shell.execute_reply.started":"2024-08-02T02:57:47.852119Z","shell.execute_reply":"2024-08-02T02:57:47.862107Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"La longitud del string más largo es: 237\n","output_type":"stream"}]},{"cell_type":"code","source":"ds.num_samples #train,valid,test","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:57:48.351574Z","iopub.execute_input":"2024-08-02T02:57:48.352494Z","iopub.status.idle":"2024-08-02T02:57:48.359749Z","shell.execute_reply.started":"2024-08-02T02:57:48.352454Z","shell.execute_reply":"2024-08-02T02:57:48.358890Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[21446, 5362, 27217]"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport numpy as np\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef atention_mask(array_sequences, max_length):\n    m = len(array_sequences)\n    atention_mask_sequence = np.zeros((m, max_length), dtype=np.float64)\n\n    for i, seq in enumerate(array_sequences):\n        seq_len = min(len(seq), max_length)\n        atention_mask_sequence[i, :seq_len] = 1\n\n    return atention_mask_sequence\n\n\ndef load_data_torchdrug(sequences, targets, ds, max_length,truncate=True):\n    print(f'{\"=\"*30}{\"DATA\":^20}{\"=\"*30}')\n    \n    n0=ds.num_samples[0] #lista que contiene el número de muestras por set:train,val,test\n    n1=ds.num_samples[0]+ds.num_samples[1]\n   \n    \n    train_sequences=[seq for seq in sequences[0:n0] if len(seq)<500]\n    train_targets=np.array([target for seq,target in zip(sequences[0:n0],targets[0:n0]) if len(seq)<500])\n  \n    val_sequences=[seq for seq in sequences[n0:n1] if len(seq)<500]\n    val_targets=np.array([target for seq,target in zip(sequences[n0:n1],targets[n0:n1]) if len(seq)<500])\n\n    test_sequences=[seq for seq in sequences[n1:] if len(seq)<500]\n    test_targets=np.array([target for seq,target in zip(sequences[n1:],targets[n1:]) if len(seq)<500])\n\n    #cnvertir a array\n    # Padear las secuencias para que todas tengan la misma longitud\n    max_len = 237\n    train_sequences = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n    val_sequences= pad_sequences(val_sequences, maxlen=max_len, padding='post')\n    test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n    \n    \n    # Crear las máscaras de atención\n    attention_mask_train = (train_sequences > 0).astype(np.float64)\n    attention_mask_val = (val_sequences > 0).astype(np.float64)\n    attention_mask_test = (test_sequences > 0).astype(np.float64)\n    \n    \n    \n    train_dataset = PeptideBERTDataset(input_ids=train_sequences, attention_masks=attention_mask_train, labels=train_targets)\n    val_dataset = PeptideBERTDataset(input_ids=val_sequences, attention_masks=attention_mask_val, labels=val_targets)\n    test_dataset = PeptideBERTDataset(input_ids=test_sequences, attention_masks=attention_mask_test, labels=test_targets)\n\n    train_data_loader = DataLoader(\n        train_dataset,\n        batch_size=16,\n        shuffle=True\n    )\n\n    val_data_loader = DataLoader(\n        val_dataset,\n        batch_size=16,\n        shuffle=False\n    )\n\n    test_data_loader = DataLoader(\n        test_dataset,\n        batch_size=16,\n        shuffle=False\n    )\n\n    print('Batch size: ', 16)\n\n    print('Train dataset samples: ', len(train_dataset))\n    print('Validation dataset samples: ', len(val_dataset))\n    print('Test dataset samples: ', len(test_dataset))\n\n    print('Train dataset batches: ', len(train_data_loader))\n    print('Validation dataset batches: ', len(val_data_loader))\n    print('Test dataset batches: ', len(test_data_loader))\n\n    print()\n\n    return train_data_loader, val_data_loader, test_data_loader","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:57:49.158153Z","iopub.execute_input":"2024-08-02T02:57:49.158951Z","iopub.status.idle":"2024-08-02T02:58:00.077896Z","shell.execute_reply.started":"2024-08-02T02:57:49.158919Z","shell.execute_reply":"2024-08-02T02:58:00.076917Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"2024-08-02 02:57:50.871300: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-02 02:57:50.871405: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-02 02:57:51.000370: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data_loader, val_data_loader, test_data_loader = load_data_torchdrug(sequences_number,targets,ds,max_length)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:58:00.079721Z","iopub.execute_input":"2024-08-02T02:58:00.080616Z","iopub.status.idle":"2024-08-02T02:58:01.176921Z","shell.execute_reply.started":"2024-08-02T02:58:00.080578Z","shell.execute_reply":"2024-08-02T02:58:01.175899Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"==============================        DATA        ==============================\nBatch size:  16\nTrain dataset samples:  21446\nValidation dataset samples:  5362\nTest dataset samples:  27217\nTrain dataset batches:  1341\nValidation dataset batches:  336\nTest dataset batches:  1702\n\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data_loader.dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:58:01.178201Z","iopub.execute_input":"2024-08-02T02:58:01.178596Z","iopub.status.idle":"2024-08-02T02:58:01.219521Z","shell.execute_reply.started":"2024-08-02T02:58:01.178562Z","shell.execute_reply":"2024-08-02T02:58:01.218677Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([ 2, 12,  0, 13, 13,  8, 16,  5,  0,  4,  4,  3,  7,  8,  4, 13,  8, 10,\n          0, 10,  4,  9,  0, 15, 12, 16,  2,  4,  2,  0, 13,  0, 13,  0, 10,  1,\n          5, 18,  0, 12,  8,  5,  8, 12, 16,  7,  6,  5,  5,  0, 12,  8,  3,  4,\n          3, 19,  3,  5,  8,  4,  5,  5,  8,  2, 18,  0,  4, 11,  6, 16,  2, 17,\n         18,  3, 10, 15, 14, 12, 11, 15, 10, 16, 16, 12,  2,  1, 14,  3, 13,  0,\n         18,  4, 11, 13, 17,  5,  7, 16, 16, 12, 10, 10,  0,  9, 18, 12,  5, 17,\n          1, 13,  4, 12, 16, 13,  0, 10,  5,  8,  4,  9, 17,  7, 13,  8, 12,  0,\n          7, 10, 16, 12, 13, 10,  0,  9,  7,  8,  0, 15, 12,  8, 13, 18,  9, 18,\n          9,  2, 15,  9,  4, 18,  7, 14,  1, 10, 12, 11, 12,  9,  0,  7, 12,  4,\n          9, 16, 12,  7, 17, 15, 12,  7, 13, 10,  0,  2,  4, 11,  8,  1, 10, 15,\n         18, 11, 11,  9,  5,  3,  7,  0, 10,  0,  3,  4,  8,  8,  3, 10,  9, 15,\n         18,  8,  2,  5, 11,  2,  1,  8,  2, 12, 10,  3,  9, 13, 12, 17, 10, 15,\n         14,  4,  8,  8, 13, 16,  4,  5,  1,  1,  0,  7,  5, 15,  0, 14, 10, 13,\n         17, 18, 12]),\n 'attention_mask': tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n         1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]),\n 'labels': tensor(3.8237)}"},"metadata":{}}]},{"cell_type":"code","source":"train_data_loader.dataset[0]['input_ids']","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:58:01.221939Z","iopub.execute_input":"2024-08-02T02:58:01.222536Z","iopub.status.idle":"2024-08-02T02:58:01.230048Z","shell.execute_reply.started":"2024-08-02T02:58:01.222503Z","shell.execute_reply":"2024-08-02T02:58:01.229193Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"tensor([ 2, 12,  0, 13, 13,  8, 16,  5,  0,  4,  4,  3,  7,  8,  4, 13,  8, 10,\n         0, 10,  4,  9,  0, 15, 12, 16,  2,  4,  2,  0, 13,  0, 13,  0, 10,  1,\n         5, 18,  0, 12,  8,  5,  8, 12, 16,  7,  6,  5,  5,  0, 12,  8,  3,  4,\n         3, 19,  3,  5,  8,  4,  5,  5,  8,  2, 18,  0,  4, 11,  6, 16,  2, 17,\n        18,  3, 10, 15, 14, 12, 11, 15, 10, 16, 16, 12,  2,  1, 14,  3, 13,  0,\n        18,  4, 11, 13, 17,  5,  7, 16, 16, 12, 10, 10,  0,  9, 18, 12,  5, 17,\n         1, 13,  4, 12, 16, 13,  0, 10,  5,  8,  4,  9, 17,  7, 13,  8, 12,  0,\n         7, 10, 16, 12, 13, 10,  0,  9,  7,  8,  0, 15, 12,  8, 13, 18,  9, 18,\n         9,  2, 15,  9,  4, 18,  7, 14,  1, 10, 12, 11, 12,  9,  0,  7, 12,  4,\n         9, 16, 12,  7, 17, 15, 12,  7, 13, 10,  0,  2,  4, 11,  8,  1, 10, 15,\n        18, 11, 11,  9,  5,  3,  7,  0, 10,  0,  3,  4,  8,  8,  3, 10,  9, 15,\n        18,  8,  2,  5, 11,  2,  1,  8,  2, 12, 10,  3,  9, 13, 12, 17, 10, 15,\n        14,  4,  8,  8, 13, 16,  4,  5,  1,  1,  0,  7,  5, 15,  0, 14, 10, 13,\n        17, 18, 12])"},"metadata":{}}]},{"cell_type":"code","source":"train_data_loader.dataset[7410]['input_ids'].size()","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:58:01.231351Z","iopub.execute_input":"2024-08-02T02:58:01.231770Z","iopub.status.idle":"2024-08-02T02:58:01.238417Z","shell.execute_reply.started":"2024-08-02T02:58:01.231723Z","shell.execute_reply":"2024-08-02T02:58:01.237579Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([237])"},"metadata":{}}]},{"cell_type":"code","source":"##Configuración y Preparación###\n#llamamos al archivo donde se guarda la config del modelo peptidebert\nconfig = yaml.load(open('/kaggle/input/proyecto-archivos/config.yaml', 'r'), Loader=yaml.FullLoader)\nconfig['task'] = 'fluoresencia'\nconfig['batch_size'] = 16\nconfig['epochs'] = 30\nconfig['optim']['lr'] = 1.0e-5\nconfig['sch']['steps'] = len(train_data_loader)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:58:01.239437Z","iopub.execute_input":"2024-08-02T02:58:01.240218Z","iopub.status.idle":"2024-08-02T02:58:01.253506Z","shell.execute_reply.started":"2024-08-02T02:58:01.240177Z","shell.execute_reply":"2024-08-02T02:58:01.252601Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def create_model_torchdrug(config):\n    bert_config = BertConfig(\n        vocab_size=25,\n        hidden_size=512,\n        num_hidden_layers=16,\n        num_attention_heads=16,\n        hidden_dropout_prob=0.15,\n        max_position_embeddings= 256 #maximo len de preentrenamiento HF\n    )\n    #creamos una istancia de PeptideBERT utilizando la configuración de BERT definida\n    model = PeptideBERT(bert_config).to(device)\n    #regresamos el modelo\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-08-02T02:58:14.859894Z","iopub.execute_input":"2024-08-02T02:58:14.860707Z","iopub.status.idle":"2024-08-02T02:58:14.865663Z","shell.execute_reply.started":"2024-08-02T02:58:14.860676Z","shell.execute_reply":"2024-08-02T02:58:14.864749Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"\n#creamos el modelo\nmodel_torchdrug = create_model_torchdrug(config)\n\n#configuramos criterio de pérdida, optimizador y scheduler\ncriterion, optimizer, scheduler = cri_opt_sch(config, model_torchdrug)\n\n\n#Configuración de Weights & Biases (WandB)\nif not config['debug']:\n    run_name = f'{config[\"task\"]}-{datetime.now().strftime(\"%m%d_%H%M\")}'\n    wandb.init(project='PeptideBERT', name=run_name)\n\n    save_dir = f'/kaggle/working/checkpoints/{run_name}'\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    else:\n        print('ya existe')\n    shutil.copy('/kaggle/input/proyecto-archivos/config.yaml', f'{save_dir}/config.yaml')\n    #shutil.copy('/kaggle/input/model-peptidos/network.py', f'{save_dir}/network.py')","metadata":{"execution":{"iopub.status.busy":"2024-08-02T03:24:46.948639Z","iopub.execute_input":"2024-08-02T03:24:46.949316Z","iopub.status.idle":"2024-08-02T03:25:10.566613Z","shell.execute_reply.started":"2024-08-02T03:24:46.949286Z","shell.execute_reply":"2024-08-02T03:25:10.565543Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:2wrg4j1x) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_spearman</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>1e-05</td></tr><tr><td>train_loss</td><td>0.78892</td></tr><tr><td>val_loss</td><td>0.71501</td></tr><tr><td>val_spearman</td><td>0.10353</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fluoresencia-0802_0313</strong> at: <a href='https://wandb.ai/sara-garcia/PeptideBERT/runs/2wrg4j1x' target=\"_blank\">https://wandb.ai/sara-garcia/PeptideBERT/runs/2wrg4j1x</a><br/> View project at: <a href='https://wandb.ai/sara-garcia/PeptideBERT' target=\"_blank\">https://wandb.ai/sara-garcia/PeptideBERT</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240802_031325-2wrg4j1x/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:2wrg4j1x). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240802_032448-qb75kizt</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sara-garcia/PeptideBERT/runs/qb75kizt' target=\"_blank\">fluoresencia-0802_0324</a></strong> to <a href='https://wandb.ai/sara-garcia/PeptideBERT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sara-garcia/PeptideBERT' target=\"_blank\">https://wandb.ai/sara-garcia/PeptideBERT</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sara-garcia/PeptideBERT/runs/qb75kizt' target=\"_blank\">https://wandb.ai/sara-garcia/PeptideBERT/runs/qb75kizt</a>"},"metadata":{}}]},{"cell_type":"code","source":"#Entrenamiento del Modelo\ntrain_model(model_torchdrug)\nif not config['debug']:\n    model_torchdrug.load_state_dict(torch.load(f'{save_dir}/model.pt')['model_state_dict'], strict=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-02T04:19:20.055839Z","iopub.execute_input":"2024-08-02T04:19:20.056484Z","iopub.status.idle":"2024-08-02T08:31:22.845266Z","shell.execute_reply.started":"2024-08-02T04:19:20.056454Z","shell.execute_reply":"2024-08-02T08:31:22.844481Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"==============================      TRAINING      ==============================\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:48<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30 - Train Loss: 0.7102146951244181\tLR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30 - Validation Loss: 0.701082642101461\t Validation spearman: 0.09626263599120631\n\nModel Saved\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:48<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/30 - Train Loss: 0.7097604602941997\tLR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/30 - Validation Loss: 0.7004825546450558\t Validation spearman: 0.24070103174442548\n\nModel Saved\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:48<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/30 - Train Loss: 0.7081815564559076\tLR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/30 - Validation Loss: 0.7075522171466478\t Validation spearman: 0.22581115338103414\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:48<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/30 - Train Loss: 0.7089868290634586\tLR: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/30 - Validation Loss: 0.7229855139961555\t Validation spearman: 0.10076756797194697\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:48<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/30 - Train Loss: 0.7063267248877831\tLR: 1.0000000000000002e-06\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/30 - Validation Loss: 0.7032019827248794\t Validation spearman: 0.12327946448778014\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:48<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/30 - Train Loss: 0.7063942842079933\tLR: 1.0000000000000002e-06\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/30 - Validation Loss: 0.7044989633640009\t Validation spearman: 0.15567046915848656\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:48<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/30 - Train Loss: 0.7070801406543171\tLR: 1.0000000000000002e-06\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/30 - Validation Loss: 0.7017252922191152\t Validation spearman: 0.17082044763159762\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/30 - Train Loss: 0.7060433746850997\tLR: 1.0000000000000002e-06\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/30 - Validation Loss: 0.6996578043991966\t Validation spearman: 0.2130033362142388\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/30 - Train Loss: 0.7069640752319788\tLR: 1.0000000000000002e-06\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/30 - Validation Loss: 0.7049830453026862\t Validation spearman: 0.20661102412607643\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/30 - Train Loss: 0.7064919169994986\tLR: 1.0000000000000002e-07\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/30 - Validation Loss: 0.7037663140688979\t Validation spearman: 0.21985307594146863\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/30 - Train Loss: 0.7075876970674723\tLR: 1.0000000000000002e-07\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/30 - Validation Loss: 0.703661255560638\t Validation spearman: 0.24095289809927192\n\nModel Saved\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/30 - Train Loss: 0.707435827660614\tLR: 1.0000000000000002e-07\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/30 - Validation Loss: 0.7029348810735557\t Validation spearman: 0.22160414135480097\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/30 - Train Loss: 0.7061256672867549\tLR: 1.0000000000000002e-07\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/30 - Validation Loss: 0.7028784810432366\t Validation spearman: 0.23042790453099976\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/30 - Train Loss: 0.7066634077853804\tLR: 1.0000000000000002e-07\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/30 - Validation Loss: 0.7035509960592857\t Validation spearman: 0.23366330660365456\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/30 - Train Loss: 0.7080022019072312\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/30 - Validation Loss: 0.7034720762943228\t Validation spearman: 0.2307565770090365\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/30 - Train Loss: 0.7041708030678904\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/30 - Validation Loss: 0.7034848143036166\t Validation spearman: 0.24270869507660983\n\nModel Saved\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/30 - Train Loss: 0.7077989210385071\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/30 - Validation Loss: 0.7034345087595284\t Validation spearman: 0.2451943574865917\n\nModel Saved\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/30 - Train Loss: 0.7073965717904342\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/30 - Validation Loss: 0.7034141504471856\t Validation spearman: 0.24048930067816604\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/30 - Train Loss: 0.7080928650222813\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/30 - Validation Loss: 0.7033784652261862\t Validation spearman: 0.25182226008585656\n\nModel Saved\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/30 - Train Loss: 0.7076198213152437\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/30 - Validation Loss: 0.7033570002808812\t Validation spearman: 0.24737080590523078\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/30 - Train Loss: 0.7061562572205182\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/30 - Validation Loss: 0.7033415256910736\t Validation spearman: 0.24387881913623788\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/30 - Train Loss: 0.7057479697208099\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/30 - Validation Loss: 0.7033251475409737\t Validation spearman: 0.24909161415913025\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/30 - Train Loss: 0.7063505232867512\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/30 - Validation Loss: 0.7033452050139507\t Validation spearman: 0.23297010865299236\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24/30 - Train Loss: 0.7066311105241267\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24/30 - Validation Loss: 0.703316040226214\t Validation spearman: 0.2445137740727179\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25/30 - Train Loss: 0.7063132333364351\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25/30 - Validation Loss: 0.7033215663174078\t Validation spearman: 0.2390703095500761\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26/30 - Train Loss: 0.7057580755027615\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26/30 - Validation Loss: 0.7033179445147869\t Validation spearman: 0.21581569918328175\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27/30 - Train Loss: 0.707142449004227\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27/30 - Validation Loss: 0.703313705378345\t Validation spearman: 0.2443833231693246\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28/30 - Train Loss: 0.7074430815147695\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28/30 - Validation Loss: 0.7032878540145854\t Validation spearman: 0.23651992556619128\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29/30 - Train Loss: 0.7075561036999594\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29/30 - Validation Loss: 0.7032708614798528\t Validation spearman: 0.24438732950996928\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1341/1341 [07:47<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30/30 - Train Loss: 0.7067271697819011\tLR: 1.0000000000000004e-08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 336/336 [00:35<00:00,  9.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30/30 - Validation Loss: 0.7032723975633937\t Validation spearman: 0.24297107155280848\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>█████████▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▄▁█▆▂▂▃▆▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>val_spearman</td><td>▅▄▄██▁▇▇▁▂▄▄▆▆▆▇▆▇▇▇▇█▇██▇█▇▇▇▆▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.70673</td></tr><tr><td>val_loss</td><td>0.70327</td></tr><tr><td>val_spearman</td><td>0.24297</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fluoresencia-0802_0324</strong> at: <a href='https://wandb.ai/sara-garcia/PeptideBERT/runs/qb75kizt' target=\"_blank\">https://wandb.ai/sara-garcia/PeptideBERT/runs/qb75kizt</a><br/> View project at: <a href='https://wandb.ai/sara-garcia/PeptideBERT' target=\"_blank\">https://wandb.ai/sara-garcia/PeptideBERT</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240802_032448-qb75kizt/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}}]},{"cell_type":"code","source":"#test\ntest_sperman = test(model_torchdrug, test_data_loader, device)\nprint(f'Test Accuracy: {test_sperman}%')","metadata":{"execution":{"iopub.status.busy":"2024-08-02T08:33:50.265743Z","iopub.execute_input":"2024-08-02T08:33:50.266353Z","iopub.status.idle":"2024-08-02T08:36:50.136937Z","shell.execute_reply.started":"2024-08-02T08:33:50.266323Z","shell.execute_reply":"2024-08-02T08:36:50.136080Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"100%|██████████| 1702/1702 [02:59<00:00,  9.46it/s]","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.23889437267209304%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}